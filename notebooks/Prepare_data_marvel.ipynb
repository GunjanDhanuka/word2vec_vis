{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b4bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy  # For preprocessing\n",
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9402ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1efb5c8-2c0c-4cc4-8b34-13100a853376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gunjan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gunjan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/gunjan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gunjan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494e3bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marvel/Iron-Man.2.txt', 'marvel/Spider-Man.Far.From.Home.txt', 'marvel/Captain.Marvel.txt']\n",
      "\n",
      "Total number of movie scripts: 23\n"
     ]
    }
   ],
   "source": [
    "txt_files = glob.glob(\"marvel/*.txt\")\n",
    "\n",
    "print(txt_files[:3], end=\"\\n\\n\")\n",
    "print(\"Total number of movie scripts:\", len(txt_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a064667",
   "metadata": {},
   "outputs": [],
   "source": [
    "marvel_corpus = \"\"\n",
    "\n",
    "for file in txt_files:\n",
    "    with open(file, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        text = f.read()\n",
    "        marvel_corpus += text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775128db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text from the corpus:\n",
      "\n",
      " urned. we're adjourned for today.\n",
      "- okay. - you've been a delight.\n",
      "my bond is with the people.\n",
      "and i will serve this great nation at the pleasure of myself.\n",
      "if there's one thing i've proven\n",
      "it's that you can count on me to pleasure myself.\n",
      "wake up. daddy's home.\n",
      "welcome home, sir.\n",
      "congratulations on the opening ceremonies.\n",
      "they were such a success, as was your senate hearing.\n",
      "and may i say how refreshing it is\n",
      "to finally see you in a video with your clothing on, sir.\n",
      "you!\n",
      "i swear to god i'll dis\n"
     ]
    }
   ],
   "source": [
    "marvel_corpus = marvel_corpus.lower()\n",
    "print(\"Sample text from the corpus:\\n\\n\", marvel_corpus[9500:10000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0119e-77db-4158-a4e4-4a71582478d4",
   "metadata": {},
   "source": [
    "## Cleaning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc77f0b-e0e6-493f-8679-6cc1ba028710",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcf73202-a5e4-40a4-9bfe-7a8128d0587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been a while since i was up here in front of you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'since front'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "text = marvel_corpus\n",
    "text = re.sub(r\"[^.A-Za-z]\", \" \", text)\n",
    "sentence_list = text.split(\".\")\n",
    "print(sentence_list[0])\n",
    "sentences_without_stopword = []\n",
    "for sentence in sentence_list:\n",
    "    temp = []\n",
    "    for word in sentence.split():\n",
    "        if word not in stop_words:\n",
    "            temp.append(word)\n",
    "    sentences_without_stopword.append(\" \".join(temp))\n",
    "sentences_without_stopword[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5533dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['since', 'front']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=[nltk.word_tokenize(words) for words in sentences_without_stopword]\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c968b0c-dc28-4b9e-b5a2-b18527157f56",
   "metadata": {},
   "source": [
    "## bIGRAMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e59a6b0b-9fcf-4c57-a157-1eb270752bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7630881b-3435-4d68-b7cb-434922e91a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:26:54: collecting all words and their counts\n",
      "INFO - 17:26:54: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 17:26:54: PROGRESS: at sentence #10000, processed 27219 words and 21466 word types\n",
      "INFO - 17:26:54: PROGRESS: at sentence #20000, processed 56151 words and 40845 word types\n",
      "INFO - 17:26:54: PROGRESS: at sentence #30000, processed 85759 words and 58899 word types\n",
      "INFO - 17:26:54: collected 67354 token types (unigram + bigrams) from a corpus of 101708 words and 36103 sentences\n",
      "INFO - 17:26:54: merged Phrases<67354 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 17:26:54: Phrases lifecycle event {'msg': 'built Phrases<67354 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000> in 0.13s', 'datetime': '2022-06-27T17:26:54.473865', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# min_count (float, optional) â€“ Ignore all words and bigrams with total collected count lower than this value.\n",
    "phrases = Phrases(tokens, min_count=20, progress_per=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f37690-8947-45ee-bec6-6e9ea9279ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:26:57: exporting phrases from Phrases<67354 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 17:26:57: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<15 phrases, min_count=20, threshold=10.0> from Phrases<67354 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000> in 0.11s', 'datetime': '2022-06-27T17:26:57.625815', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d366405c-7db3-4307-8aaa-3b7cef2fc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04860170-3a26-4811-96ae-e5bd6cf94ece",
   "metadata": {},
   "source": [
    "### Most frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6d494a-fc29-4097-a27f-0b439cfdc4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11178"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for word in sent:\n",
    "        word_freq[word] += 1\n",
    "len(word_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09fd71e2-ca67-4c4a-b675-eab7668cd054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['know', 'right', 'get', 'yeah', 'like', 'one', 'got', 'okay', 'gon_na', 'go']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53060dfb-1cb4-4cb8-847d-7030bc235a1f",
   "metadata": {},
   "source": [
    "## Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a20e82db-e608-41cf-be30-38ec9beabb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f9a7d2e-ec64-4e72-b288-7c360b7dd820",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()  # Count the number of cores in a computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78366fde-9983-4360-b7e4-b4b56e901004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:28:54: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2022-06-27T17:28:54.594144', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5,\n",
    "                     sg=1,\n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf8114ce-6a56-48a9-8e6e-2711407d0eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:28:55: collecting all words and their counts\n",
      "INFO - 17:28:55: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 17:28:55: PROGRESS: at sentence #10000, processed 26761 words, keeping 5350 word types\n",
      "INFO - 17:28:55: PROGRESS: at sentence #20000, processed 55327 words, keeping 8187 word types\n",
      "INFO - 17:28:55: PROGRESS: at sentence #30000, processed 84479 words, keeping 10338 word types\n",
      "INFO - 17:28:55: collected 11178 word types from a corpus of 100068 raw words and 36103 sentences\n",
      "INFO - 17:28:55: Creating a fresh vocabulary\n",
      "INFO - 17:28:55: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 866 unique words (7.75% of original 11178, drops 10312)', 'datetime': '2022-06-27T17:28:55.802218', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 17:28:55: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 68128 word corpus (68.08% of original 100068, drops 31940)', 'datetime': '2022-06-27T17:28:55.803259', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 17:28:55: deleting the raw counts dictionary of 11178 items\n",
      "INFO - 17:28:55: sample=6e-05 downsamples 866 most-common words\n",
      "INFO - 17:28:55: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 17252.40528250482 word corpus (25.3%% of prior 68128)', 'datetime': '2022-06-27T17:28:55.811449', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'prepare_vocab'}\n",
      "INFO - 17:28:55: estimated required memory for 866 words and 300 dimensions: 2511400 bytes\n",
      "INFO - 17:28:55: resetting layer weights\n",
      "INFO - 17:28:55: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-06-27T17:28:55.827650', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary table\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print(\"Time to build vocab: {} mins\".format(round((time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ff646-31d9-4dd5-9aca-b8f40ac64eac",
   "metadata": {},
   "source": [
    "### Training of model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7423a5c-59db-4151-b05e-af006cd0d9df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 17:30:47: Effective 'alpha' higher than previous training cycles\n",
      "INFO - 17:30:47: Word2Vec lifecycle event {'msg': 'training model with 11 workers on 866 vocabulary and 300 features, using sg=1 hs=0 sample=6e-05 negative=20 window=5 shrink_windows=True', 'datetime': '2022-06-27T17:30:47.149422', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'train'}\n",
      "INFO - 17:30:47: EPOCH 0: training on 100068 raw words (17217 effective words) took 0.2s, 93866 effective words/s\n",
      "INFO - 17:30:47: EPOCH 1: training on 100068 raw words (17292 effective words) took 0.2s, 91929 effective words/s\n",
      "INFO - 17:30:47: EPOCH 2: training on 100068 raw words (17356 effective words) took 0.2s, 102355 effective words/s\n",
      "INFO - 17:30:48: EPOCH 3: training on 100068 raw words (17237 effective words) took 0.3s, 53461 effective words/s\n",
      "INFO - 17:30:48: EPOCH 4: training on 100068 raw words (17083 effective words) took 0.2s, 95196 effective words/s\n",
      "INFO - 17:30:48: EPOCH 5: training on 100068 raw words (17169 effective words) took 0.2s, 95534 effective words/s\n",
      "INFO - 17:30:48: EPOCH 6: training on 100068 raw words (17401 effective words) took 0.2s, 90380 effective words/s\n",
      "INFO - 17:30:48: EPOCH 7: training on 100068 raw words (17132 effective words) took 0.2s, 96655 effective words/s\n",
      "INFO - 17:30:49: EPOCH 8: training on 100068 raw words (17091 effective words) took 0.2s, 94174 effective words/s\n",
      "INFO - 17:30:49: EPOCH 9: training on 100068 raw words (17248 effective words) took 0.3s, 55000 effective words/s\n",
      "INFO - 17:30:49: EPOCH 10: training on 100068 raw words (17265 effective words) took 0.2s, 100098 effective words/s\n",
      "INFO - 17:30:49: EPOCH 11: training on 100068 raw words (17147 effective words) took 0.2s, 99023 effective words/s\n",
      "INFO - 17:30:49: EPOCH 12: training on 100068 raw words (17239 effective words) took 0.2s, 100655 effective words/s\n",
      "INFO - 17:30:50: EPOCH 13: training on 100068 raw words (17114 effective words) took 0.2s, 106599 effective words/s\n",
      "INFO - 17:30:50: EPOCH 14: training on 100068 raw words (17268 effective words) took 0.3s, 55523 effective words/s\n",
      "INFO - 17:30:50: EPOCH 15: training on 100068 raw words (16999 effective words) took 0.2s, 95134 effective words/s\n",
      "INFO - 17:30:50: EPOCH 16: training on 100068 raw words (17232 effective words) took 0.2s, 101885 effective words/s\n",
      "INFO - 17:30:50: EPOCH 17: training on 100068 raw words (17234 effective words) took 0.2s, 114053 effective words/s\n",
      "INFO - 17:30:51: EPOCH 18: training on 100068 raw words (17362 effective words) took 0.2s, 101635 effective words/s\n",
      "INFO - 17:30:51: EPOCH 19: training on 100068 raw words (17333 effective words) took 0.3s, 52711 effective words/s\n",
      "INFO - 17:30:51: EPOCH 20: training on 100068 raw words (17296 effective words) took 0.2s, 100410 effective words/s\n",
      "INFO - 17:30:51: EPOCH 21: training on 100068 raw words (17064 effective words) took 0.2s, 98379 effective words/s\n",
      "INFO - 17:30:51: EPOCH 22: training on 100068 raw words (17183 effective words) took 0.2s, 102308 effective words/s\n",
      "INFO - 17:30:52: EPOCH 23: training on 100068 raw words (17187 effective words) took 0.2s, 103039 effective words/s\n",
      "INFO - 17:30:52: EPOCH 24: training on 100068 raw words (17491 effective words) took 0.2s, 106505 effective words/s\n",
      "INFO - 17:30:52: EPOCH 25: training on 100068 raw words (17270 effective words) took 0.3s, 52880 effective words/s\n",
      "INFO - 17:30:52: EPOCH 26: training on 100068 raw words (17331 effective words) took 0.2s, 76724 effective words/s\n",
      "INFO - 17:30:53: EPOCH 27: training on 100068 raw words (17290 effective words) took 0.2s, 99902 effective words/s\n",
      "INFO - 17:30:53: EPOCH 28: training on 100068 raw words (17141 effective words) took 0.2s, 103418 effective words/s\n",
      "INFO - 17:30:53: EPOCH 29: training on 100068 raw words (17035 effective words) took 0.2s, 96550 effective words/s\n",
      "INFO - 17:30:53: Word2Vec lifecycle event {'msg': 'training on 3002040 raw words (516707 effective words) took 6.3s, 82493 effective words/s', 'datetime': '2022-06-27T17:30:53.414118', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.1 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(\n",
    "    sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1\n",
    ")\n",
    "\n",
    "print(\"Time to train the model: {} mins\".format(round((time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8a5d02a-e053-42c0-aca4-a244885d890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:32:35: Word2Vec lifecycle event {'fname_or_handle': 'marvel_w2v.bin', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-06-27T17:32:35.947632', 'gensim': '4.2.0', 'python': '3.8.0 (default, Nov  6 2019, 21:49:08) \\n[GCC 7.3.0]', 'platform': 'Linux-5.13.0-51-generic-x86_64-with-glibc2.10', 'event': 'saving'}\n",
      "INFO - 17:32:35: not storing attribute cum_table\n",
      "INFO - 17:32:35: saved marvel_w2v.bin\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"marvel_w2v.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea063c6-1e9b-4c68-ac48-7ae2ddf0fa2d",
   "metadata": {},
   "source": [
    "## Exploring the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06a5494b-945c-48c2-aea5-e1393e16b742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('throne', 0.9986668229103088),\n",
       " ('king', 0.998475968837738),\n",
       " ('loki', 0.9984367489814758),\n",
       " ('father', 0.998172402381897),\n",
       " ('without', 0.9980611205101013),\n",
       " ('destroyed', 0.9980530738830566),\n",
       " ('came', 0.9980035424232483),\n",
       " ('yet', 0.997901201248169),\n",
       " ('war', 0.9978365898132324),\n",
       " ('much', 0.9978256225585938)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the word is in the vocabulary before feeding in\n",
    "\n",
    "# Get most similar words\n",
    "w2v_model.wv.most_similar(positive=[\"\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bdd323c8-6065-4734-8606-64e48f828853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46241245"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similarity(\"batman\", \"joker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "23da6ccc-8596-459d-bbb0-a718ec9decb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hulk'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match([\"batman\", \"joker\", \"hulk\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33ea0cf4-82b9-4803-a2d7-8098a3752cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bridge', 0.28366515040397644),\n",
       " ('kleenex', 0.28308799862861633),\n",
       " ('port', 0.27012956142425537)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\n",
    "    positive=[\"titanic\", \"james_cameron\"], negative=[\"christopher_nolan\"], topn=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62ddf782-efe0-4ffb-b1e0-eeddc178fe41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/gunjan/Desktop/word2vec_project/Prepare_data_marvel.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gunjan/Desktop/word2vec_project/Prepare_data_marvel.ipynb#ch0000038?line=0'>1</a>\u001b[0m voc \u001b[39m=\u001b[39m w2v_model\u001b[39m.\u001b[39;49mwv\u001b[39m.\u001b[39;49mvocab\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gunjan/Desktop/word2vec_project/Prepare_data_marvel.ipynb#ch0000038?line=1'>2</a>\u001b[0m \u001b[39mlen\u001b[39m(voc)\n",
      "File \u001b[0;32m~/anaconda3/envs/w2v/lib/python3.8/site-packages/gensim/models/keyedvectors.py:735\u001b[0m, in \u001b[0;36mKeyedVectors.vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvocab\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    736\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe vocab attribute was removed from KeyedVector in Gensim 4.0.0.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse KeyedVector\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms .key_to_index dict, .index_to_key list, and methods \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    740\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "voc = w2v_model.wv.vocab\n",
    "len(voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaed3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "296e912957bd482cc6e88dcab6e634628d0861d6347ba5ff2b91c085c9501fc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
